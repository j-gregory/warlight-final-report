\documentclass[a4paper,11pt]{article}

\usepackage[margin=1.0in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{todonotes}

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\title{I Came, I Saw, I Planned, I Conquered: \\ A Plan for World Domination}
\author{Jason Gregory, Moshe Katz, \& Kamruzzaman Quddus \\ \{jgregory, mmkatz, kquddus\}@umd.edu}
\affil{University of Maryland, College Park, MD}
\date{\today}

\maketitle

\abstract{In this work, we describe a probabilistic planner that competes in
the Warlight AI Challenge 2.  More specifically, we have implemented and modified
the \emph{Upper Confidence Bounds on Trees} (UCT) algorithm for winning Warlight
games when randomly paired up against users with other strategies.}

\section{Introduction}\label{sec:intro}
\subsection{AI in Games}\label{aiingames}
Although there are many major commercial fields that benefit from improvements
in Artificial Intelligence (AI), one of the largest research areas in the field is
the application to games.  From the 1951 debut of artificially intelligent bots
for Nim \cite{nim}, checkers, and chess \cite{checkerschess}, to Deep Blue's 1997
defeat of chess grandmaster Garry Kasparov, and continuing to today's sophisticated
first-person-shooter adversaries, the impact and importance of AI in gaming as a
tool for understanding and furthering AI techniques for application to real-world 
problems cannot be overstated.

Despite all of this work, there still remains many "open problems" in game AI
design.  Many games still use AI bots that are quite naive, and some games have
not yet been determined to be winnable by a computer player. To encourage continued
research and development of game AI, some game makers occasionally run competitions
for developing new AI bots to play their games.

In this work, we have developed a bot to compete in the Warlight AI Challenge 2 
\cite{warlight}. The goal of this competition is to build a bot that can play (and, of 
course, can win) Warlight, a RISK-style game.

\subsection{Warlight} \label{sec:warlight}
For a full discussion of the rules and origins of the game \emph{Warlight}, refer
to our previous paper, \emph{Towards the Planning of World Domination}\cite{ourproposal}.
To recap the major points, Warlight is a game similar to the classic board game RISK, with 
the addition of fog-of-war and randomly-generated maps. The Warlight AI Challenge 2 is a 
competition that pits bots against each other in a series of two-player games and collects 
the scores of the games to determine a winner of the competition.

\section{Related Work}\label{sec:previous}
The largest body of related work is the set of other bots in the current challenge.  
There are currently around 200 competition entries from 30 countries written in 
eleven languages \cite{warlight}.  While we have not examined many of our competitors 
in detail, they are all working toward the same goal as we are of developing an AI bot for 
this game, and some of them are very good at playing the game.

Additional related work comes from the first Warlight AI Challenge.  While the 
rules of the game for that competition were slightly different, many of the 
techniques used there will still apply to this challenge, either as examples of 
successful approaches, or otherwise as examples of what not to try.

There are existing AI bots for the original RISK game.  While some have been 
discussed only theoretically, including strategies from MIT \cite{riskmit}, Markov 
Chains from North Carolina State University \cite{riskncs}, Monte-Carlo (UCT) 
techniques for territory selection from University of Alberta \cite{riskalb}, and 
studies of dice-rolls from Elon University \cite{riskelon}, implementations of AI 
players for RISK have been created and sold in commercial software as early as 1989. 
These solutions are also of varying quality - the 1992 release of \textit{WinRisk} for 
Windows 3.1 was capable of beating at-best an inexpert child player - and most 
implementations are closed-source without any documentation of their algorithms.

The UCT algorithm has been used as the basis for other games: the algorithm was originally 
described for the game \emph{Go}\cite{uct}, then also applied to 
\emph{backgammon}\cite{uctback}.  However, unlike Warlight, in both of those games, the game 
itself is played on a board of finite size and both players have full knowledge of the state 
of the board.  In contrast, Warlight's random maps mean that we cannot know anything about 
the board size and layout beforehand, while Warlight's for of war means that we do not have 
perfect knowledge of the board. UCT has also been used for more complex games, such as 
Tantrix\cite{tantrix} which has a much more dynamic board than Go and backgammon.

\section{Approach}\label{sec:approach}
We have built a bot that uses \emph{Monte-Carlo Tree Search} (MCTS) and \emph{Upper 
Confidence Bound for Trees} (UCT) to play Warlight.  We have also built an ``Opponent Bot'' 
that performs its moves greedily instead of using our algorithm.

\subsection{UCT and MCTS}
While traditional AI algorithms for playing games rely on static evaluation functions that 
analyze the game tree to determine the next move to make based on the current state of the 
game, Monte-Carlo Tree Search uses an entirely different approach.  Taking into account that 
the game tree of complex games may be too big to analyze completely for every move, MCTS 
uses a cycle of \emph{Selection, Expansion, Simulation, and Back-Propagation} to selectively 
explore the game tree.

\subsubsection{Selection}
The first step of MCTS is finding a leaf node of the game tree from which to further 
analyze.  The choice of which leaf node will be selected must balance the two goals of 
\emph{exploration} and \emph{expansion} -- whether to strike off in a new direction in the tree to see if 
the result will be better or to press farther down the current path.

Within the MCTS ``Selection'' step, we use UCT to determine the ideal node for 
selection.  We apply the UCT algorithm to determine a score for each node, then choose the 
node with the best score.  For information about the UCT algorithm, see Section~\ref{sec:uct}.

\subsubsection{Expansion}
Once a leaf node has been selected, the Expansion process begins. For this, 
the algorithm computes a random move, generates the resulting state by simulating the random move, 
and adds this new node to the game tree.
%one or more additional nodes are added below it to make 
%space for the game simulations that will be played based on the game state at this node.

\subsubsection{Simulation}
After a single node is added, a game of alternating turns between the user's bot and an 
opponent bot is simulated starting from the state of the node created in the Expansion step.  
%We use our OpponentBot (described in section \ref{sec:oppbot}) to simulate the opponent's moves 
%during this step.  
We first used a StarterBot, which is the baseline code that the competition provides for simulating
the opponent's move, and then later developed a more sophisticated OpponentBot, described in
Section~\ref{sec:oppbot}.  The simulation is deliberately simple -- simpler 
than our primary planning algorithm -- in order to leave the bulk of our limited execution 
time for the MCTS and UCT operations.  Note, due to time restrictions imposed by the game engine, 
we currently simulate approximately 10 turns and then stop.  Provided we have the time, we could
simulate more iterations to gain more knowledge about the path we are taking.

\subsubsection{Back-Propagation}
After a simulation is complete, the node at which the simulation was executed, and all of its 
ancestors in the tree, are updated with a measure of how successful traversing this branch was 
for our bot. Since we do not always simulate to the end of a game, we cannot simply use the measure
of whether the game was a win, loss, or draw.  In the event, the branch we traversed resulted in a win
or loss, we certainly must use this information; however, we did not rely on simulating to this point
in order to evaluate moves. In fact, we developed several metrics for which to evaluate
the quality of a branch in the game tree, which include 1) the ratio between the number of regions
the user bot owns and the total number of regions in the game; 2) the ratio between the number of 
regions the user bot owns  and the number of regions we believe the opponent owns; and 3) . 
Ultimately, we used the first metrics because of its ability to capture growth of power and 
its simplicity to calculate.  

%record of whether that game was a win, a loss, or a 
%draw.  This information is used by UCT in the future rounds' selection steps.

\subsection{UCT}\label{sec:uct}
To balance the need for exploration of entirely new paths through the game tree with
exploitation of known paths, the UCT algorithm maintains an \emph{Exploration Bias} that grows
over time, as well as a \emph{Win Rate} for each node.  The UCT score for a node is based on a
combination of the exploration bias and win rate.
\todo{Jason: Can you expand this a bit?}

\subsection{Opponent Bot}\label{sec:oppbot}
Most existing UCT literature and applications simulate both the AI player's and opponent's
moves, requiring complete visibility of the current state of the game; an expectation that
is often taken for granted by MCTS and UCT algorithm designers. However, in the Warlight 
Challenge, the only visibility UCT-based bots have is the regions which are immediately 
adjacent to its owned regions on the board. To compensate for the added complexity of 
implementing UCT with imperfect knowledge, a separate bot class called ``OpponentBot'' is 
designed to mimic the behavior of the opponent and thus work around the lack of full game 
board visibility.

The OpponentBot is designed as a greedy, depth first search-like algorithm with constraints 
(troops per turn, defined number of regions it owns and can operate within) and the requirement 
to be as run-time inexpensive as possible. Given its current list of regions, the algorithm calculates
the cost of obtaining super regions given its present state and prioritizes its plan of
region acquisition accordingly. The plan is adjusted each turn given the known state of the 
game at that turn. Because the DFS algorithm is meant to be an approximation of what an
actual opponent might have been doing both behind the `fog of war' and within visible area, 
its internal hypothetical state must be regularly updated with the actual state changes
provided by the game engine.  The update mechanism operates via internal state recalculations 
triggered by event handlers listening to game engine broadcast messages. While the
opponent bot algorithm was intended to complement our implementation of UCT, we have found 
that it may yet be robust enough to be a stand-alone bot agent. By estimating how a real 
opponent might behave, it allows us to maintain the classic model of UCT, including the 
assumption of full knowledge of the game.

\section{Implementation}\label{sec:impl}
Our bot is implemented in C++, and the code is publicly available on Github\cite{github}.  
The bot takes input on \code{STDIN} and returns commands to be executed back to the game 
engine on \code{STDOUT}.  All of the algorithms (MCTS, UCT, and the Greedy OpponentBot) are 
implemented in classes in the codebase, the first two in a class called \code{MCTSManager} 
and the latter in \code{OpponentBot}.  Additional classes are included for parsing the text 
input from the game engine and for storing the state of gameplay.  The input parser, basic 
outline of the bot code, and the classes for watching the bot state are all based on the 
starter code provided by the Warlight developers, referred to as StarterBot.  To allow ourselves 
to focus on the algorithms, we chose to use a tree structure library called \emph{tree.hh}\cite{treehh}.

\section{Experiments}\label{sec:experiments}
The simplest experiment for our bot was to submit it to the challenge server and see how it 
fared against other bots.  We have tested our bot against multiple other bots participating 
in the challenge, and we have won against several of them.  However, some of this success is 
due to the fact that many of the bots that have been uploaded are likely incomplete or are 
even just the unmodified StarterBot, and these results may be unreliable.

We are building an experiment in which we pit our MCTS/UCT bot against our greedy 
OpponentBot to determine how much better we can do with the intelligent bot than the greedy 
one.  However, we do not yet have results of this match.

Currently, the greedy agent performs well as a stand-alone agent (in test environment) 
independent of UCT; however it is currently still going through some integration issues with 
UCT. Originally developed to be run-time inexpensive, so that UCT implementation might make 
the most of pre-allocated runtime constraints imposed by game engine, the greedy algorithm 
was not designed with the design flexibility of modern object oriented language and was more 
tightly coupled by multiple container stacks and synchronizing event handler type routines. 
Thus complete integration of state classes employed by parent UCT algorithm is taking a 
little more time than the proposed time frame and remains a work in progress. However 
tangential the development of the DFS algorithm might have been within the scope of an UCT 
algorithm, the appeal of developing complementary planning algorithm as an academic exercise 
was simply too appealing to ignore.

\section{Conclusion}\label{sec:conclusion}
\todo{still needs to be filled in\ldots}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
