\documentclass[a4paper,11pt]{article}

\usepackage[margin=1.0in]{geometry}
\usepackage[affil-it]{authblk}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{todonotes}
\usepackage[titletoc,title]{appendix}

\newcommand{\code}[1]{\texttt{#1}}

\begin{document}

\title{I Came, I Saw, I Planned, I Conquered: \\ A Plan for World Domination}
\author{Jason Gregory, Moshe Katz, \& Kamruzzaman Quddus \\ \{jgregory, mmkatz, kquddus\}@umd.edu}
\affil{University of Maryland, College Park, MD}
\date{\today}

\maketitle

\abstract{In this work, we describe a probabilistic planner that competes in
the Warlight AI Challenge 2.  More specifically, we have implemented and modified
the \emph{Upper Confidence Bounds on Trees} (UCT) algorithm for winning Warlight
games when randomly paired up against users with other strategies.}

\section{Introduction}\label{sec:intro}
\subsection{AI in Games}\label{aiingames}
Although there are many major commercial fields that benefit from improvements
in Artificial Intelligence (AI), one of the largest research areas in the field is
the application to games.  From the 1951 debut of artificially intelligent bots
for Nim \cite{nim}, checkers, and chess \cite{checkerschess}, to Deep Blue's 1997
defeat of chess grandmaster Garry Kasparov, and continuing to today's sophisticated
first-person-shooter adversaries, the impact and importance of AI in gaming as a
tool for understanding and furthering AI techniques for application to real-world 
problems cannot be overstated.

Despite all of this work, there still remains many "open problems" in game AI
design.  Many games still use AI bots that are quite naive, and some games have
not yet been determined to be winnable by a computer player. To encourage continued
research and development of game AI, some game makers occasionally run competitions
for developing new AI bots to play their games.

In this work, we have developed a bot to compete in the Warlight AI Challenge 2 
\cite{warlight}. The goal of this competition is to build a bot that can play (and, of 
course, can win) Warlight, a RISK-style game.

\subsection{Warlight} \label{sec:warlight}
For a full discussion of the rules and origins of the game \emph{Warlight}, refer
to our previous paper, \emph{Towards the Planning of World Domination}\cite{ourproposal}.
To recap the major points, Warlight is a game similar to the classic board game RISK, with 
the addition of fog-of-war and randomly-generated maps. The Warlight AI Challenge 2 is a 
competition that pits bots against each other in a series of two-player games and collects 
the scores of the games to determine a winner of the competition.

\section{Related Work}\label{sec:previous}
The largest body of related work is the set of other bots in the current challenge.  
There are currently around 200 competition entries from 30 countries written in 
eleven languages \cite{warlight}.  While we have not examined many of our competitors 
in detail, they are all working toward the same goal as we are of developing an AI bot for 
this game, and some of them are very good at playing the game.

Additional related work comes from the first Warlight AI Challenge.  While the 
rules of the game for that competition were slightly different, many of the 
techniques used there will still apply to this challenge, either as examples of 
successful approaches, or otherwise as examples of what not to try.

There are existing AI bots for the original RISK game.  While some have been 
discussed only theoretically, including strategies from MIT \cite{riskmit}, Markov 
Chains from North Carolina State University \cite{riskncs}, Monte-Carlo (UCT) 
techniques for territory selection from University of Alberta \cite{riskalb}, and 
studies of dice-rolls from Elon University \cite{riskelon}, implementations of AI 
players for RISK have been created and sold in commercial software as early as 1989. 
These solutions are also of varying quality - the 1992 release of \textit{WinRisk} for 
Windows 3.1 was capable of beating at-best an inexpert child player - and most 
implementations are closed-source without any documentation of their algorithms.

The UCT algorithm has been used as the basis for other games: the algorithm was originally 
described for the game \emph{Go}\cite{uct}, then also applied to 
\emph{backgammon}\cite{uctback}.  However, unlike Warlight, in both of those games, the game 
itself is played on a board of finite size and both players have full knowledge of the state 
of the board.  In contrast, Warlight's random maps mean that we cannot know anything about 
the board size and layout beforehand, while Warlight's for of war means that we do not have 
perfect knowledge of the board. UCT has also been used for more complex games, such as 
Tantrix\cite{tantrix} which has a much more dynamic board than Go and backgammon.

\section{Approach}\label{sec:approach}
We have built a bot that uses \emph{Monte-Carlo Tree Search} (MCTS) and \emph{Upper 
Confidence Bound for Trees} (UCT) to play Warlight.  We have also built an ``Opponent Bot'' 
that performs its moves greedily instead of using our algorithm.

\subsection{MCTS}
While traditional AI algorithms for playing games rely on static evaluation functions that 
analyze the game tree to determine the next move to make based on the current state of the 
game, Monte-Carlo Tree Search uses an entirely different approach.  Taking into account that 
the game tree of complex games may be too big to analyze completely for every move, MCTS 
uses a cycle of \emph{Selection, Expansion, Simulation, and Back-Propagation} to build and selectively 
explore the game tree.  At a high level, MCTS samples the search space within a game by simulating
particular branches of the game tree to arrive at a plan.  These are referred to as a Monte Carlo 
rollout policy. As MCTS simulates and evaluates the branches of the game tree, the algorithm 
iteratively adds new states from which it can plan over.  It is important to notice the distinction
between these planning states, which MCTS creates based on simulations, and the states of the game, 
which are generated by the game engine as a result of executing moves.  The MCTS algorithm starts its
planning process based on the game engine's state, i.e. actor's state, and produces new states that
could come true if a particular action were taken.

\subsubsection{Selection}
The first step of MCTS is finding a node of the game tree from which to further 
analyze.  The choice of which node will be selected must balance the two goals of 
\emph{exploration} and \emph{expansion} -- whether to strike off in a new direction in the 
tree to see if the result will be better or to press farther down the current path.

Within the MCTS ``Selection'' step, we use UCT to determine the ideal node for 
selection.  We apply the UCT algorithm to determine a score for each node, then choose the 
node with the best score.  For information about the UCT algorithm, see Section~\ref{sec:uct}.

\subsubsection{Expansion}
Once a node has been selected, the Expansion process begins. For this, 
the algorithm computes a random move, generates the resulting state by simulating the random move, 
and adds this new node to the game tree. The random move could consist of doing nothing or any 
number of attacks and transfers between regions.  Additional detail on our approach for generating
a move is described in Section~\ref{sec:move_gen}.

\subsubsection{Simulation}
After a single node is added, a game of alternating turns between the user's bot and an 
opponent bot is simulated starting from the state of the node created in the Expansion step.  
%We use our OpponentBot (described in section \ref{sec:oppbot}) to simulate the opponent's moves 
%during this step.  
Each player generates a random move, which is then simulated to produced a new resulting state.
To simulate a move, we copied the battle simulation code directly from the open-source game engine
to ensure our results were as probabilistically correct as possible. In terms of generating moves
for the opponent, we first used a StarterBot, which is the baseline code that the competition 
organizers initially provide.  In an attempt to improve our implementation of MCTS, we developed 
a more sophisticated OpponentBot, described in Section~\ref{sec:oppbot}.  The implementation of the
OpponentBot is deliberately simple -- simpler than our primary planning algorithms -- in order to 
leave the bulk of our limited execution time for the MCTS and UCT operations.  Note, due to time 
restrictions imposed by the game engine, we currently simulate approximately 10 turns and then stop.  
Provided we have extra time during the game, we could simulate more iterations to gain additional
 knowledge about the path we are taking.

\subsubsection{Back-Propagation}
After a simulation is complete, the node at which the simulation was executed, and all of its 
ancestors in the tree, are updated with a measure of how successful traversing this branch was 
for our bot. Since we do not always simulate to the end of a game, we cannot simply use the measure
of whether the game was a win, loss, or draw.  In the event, the branch we traversed resulted in a win
or loss, we certainly must use this information; however, we did not rely on simulating to this point
in order to evaluate moves. In fact, we developed several metrics for which to evaluate
the quality of a branch in the game tree, which include 1) the ratio between the number of regions
the user bot owns and the total number of regions in the game; 2) the ratio between the number of 
regions the user bot owns  and the number of regions we believe the opponent owns; and 
3) \todo{Add additional metrics}. 
Ultimately, we used the first metrics because of its ability to capture growth of power and 
its simplicity to calculate.  

%record of whether that game was a win, a loss, or a draw.  
%This information is used by UCT in the future rounds' selection steps.

\subsection{UCT}\label{sec:uct}
The UCT algorithm provides an efficient method for sampling the search space of the game tree and 
determining where the MCTS algorithm should focus its planning efforts. To balance the need for 
exploration of entirely new paths through the game tree with exploitation of known paths, the UCT 
algorithm maintains an exploration bias that grows over time, as well as a performance score for each node. 
This performance score relates how much a particular action contributes to winning a game.
We implemented this algorithm exactly as shown in the pseudo code provided in Figure~\ref{fig:uct_alg}, 
\cite{Nau2015}, \cite{NauLecture}. Here $n(s)$, $n(s,a)$, and $n(s,\tilde{a})$ correspond to the number of 
times the UCT algorithm has visited a particular state, which contributes to the calculation of the 
exploration bias, $(\frac{log(n(s))}{n(s,a)}^{\frac{1}{2}})$. In our implementation, we chose a weighting 
factor, $C$, of value $2$ to replicate the objective function used in the Upper Confidence Bound (UCB) 
algorithm, which UCT is derived from. Finally, the $Q(\cdot, \cdot)$ value is the performance score for
a given (state, action)-pair.  This provides the functionality of exploiting actions that lead to more 
favorable states.

%
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.45\columnwidth]{uct_algorithm}
  \caption{The UCT algorithm used in this work \cite{Nau2015}, \cite{NauLecture}.}
  \label{fig:uct_alg}
\end{figure}
%


\subsection{Random Move Generation}\label{sec:move_gen}
The generation of moves for MCTS consists of either choosing to do nothing, attacking a neutral 
or enemy region, or transferring troops to an already-owned region. The overall performance of 
MCTS and UCT appears to rely heavily on the quality of random move selection and, while we want to
explore the full spectrum of actions, we also want to ensure we are only evaluating moves that are
sensible to increase our chances of winning. As a result, we explored several options for the 
generation of random moves. Intiutively, our first implementation of simulating a move involves total 
randomness.  That is, we randomly select a single region that we own, randomly select one of its 
neighbors, and issue a command to send all of the owned region's troops to the neighboring state.  
If the neighboring region also happens to be owned by our bot then this simply results in a transfer 
of troops; however, if the neighboring region is neutral or owned by the opponent we will attack it. 
In some instances, this results in needless transfers or, in the worst case, a region with a small 
number of troops attacking a neighbor with a large number of troops, which are very undesirable 
characteristics. 

The second approach we took to move generation improves this technique slightly by selecting one of 
our regions that has the largest number of troops and attacking or transferring to a random neighbor.  
This modification increases the probability of executing an intelligent action, but still leaves room 
for improvement.

Our third implementation of random move generation consists of sorting all of our owned regions
by the number of troops in a decreasing order and, for each owned region, searches for a neighboring
region that we do not own. This ensures that our region with the largest concentration of troops will
always attack a neighboring region. While this approach removes the unnecessary transfers between 
already-owned regions, it also removes all transfers, which could be undesirable late in the game
when a transfer could help a region survive an attack. 

Our fourth, and final implementation of move generation, provided a monumental improvement.  The 
approach is as follows: for every region that we own and has at least 3 troops, sort its neighbors 
by decreasing number of troops, and find the neighboring region that we do not own and has the smallest
 number of troops. If there exists no eligible, neighboring region, simply transfer to the region with 
the smallest number of troops.  If there is, however, a neighboring region that we could attack, we 
quickly evaluate whether this is  an intelligent action.  More specifically, if our region has less 
than or equal to the number of troops in the neighboring region, we refrain from attacking.  If our 
region has approximately 1.5 times the number of armies as the neighboring region then we attack with 
that many troops as this is probabilistically the number of troops required for a successful attack. 
Finally, if we do not have 1.5 times the number of troops as the opposing region, we attack with all 
of our troops to maximize success. This approach has several enticing properties.  First, and 
foremost, we iterate all of our owned regions and compute an attack or transfer.  These results are 
concatenated together and provided to the game engine so that we perform any number of attacks or 
transfers in a single move.  The addition of multiple moves per turn appears to be the largest 
contributing factor in terms of overall performance and winning games. A second desirable 
characteristic of this approach is that it efficiently evaluates the quality of an attack before blindly
executing it.  This elimates all unfavorable attacks and balances the number of transfers. 
Additionally, by calculating how many more troops our region contains than the opposing region, we 
ensure that we only attack with what we believe to be the minimum number of troops.  From this, our bot
maximizes the probability of a successful attack, while still leaving troops that are unnecessary for
the attack in their original region so that they can provide support for defending.

\subsection{Opponent Bot}\label{sec:oppbot}
Most existing UCT literature and applications simulate both the AI player's and opponent's
moves, requiring complete visibility of the current state of the game; an expectation that
is often taken for granted by MCTS and UCT algorithm designers. However, in the Warlight 
Challenge, the only visibility UCT-based bots have is the regions which are immediately 
adjacent to its owned regions on the board. To compensate for the added complexity of 
implementing UCT with imperfect knowledge, a separate bot class called ``OpponentBot'' is 
designed to mimic the behavior of the opponent and thus work around the lack of full game 
board visibility.

The OpponentBot is designed as a greedy, depth first search-like algorithm with constraints 
(troops per turn, defined number of regions it owns and can operate within) and the requirement 
to be as run-time inexpensive as possible. Given its current list of regions, the algorithm calculates
the cost of obtaining super regions given its present state and prioritizes its plan of
region acquisition accordingly. The plan is adjusted each turn given the known state of the 
game at that turn. Because the DFS algorithm is meant to be an approximation of what an
actual opponent might have been doing both behind the `fog of war' and within visible area, 
its internal hypothetical state must be regularly updated with the actual state changes
provided by the game engine.  The update mechanism operates via internal state recalculations 
triggered by event handlers listening to game engine broadcast messages. While the
opponent bot algorithm was intended to complement our implementation of UCT, we have found 
that it may yet be robust enough to be a stand-alone bot agent. By estimating how a real 
opponent might behave, it allows us to maintain the classic model of UCT, including the 
assumption of full knowledge of the game.

\section{Implementation}\label{sec:impl}
Our bot is implemented in C++, and the code is publicly available on Github\cite{github}.  
The bot takes input on \code{STDIN} and returns commands to be executed back to the game 
engine on \code{STDOUT}.  All of the algorithms (MCTS, UCT, and the Greedy OpponentBot) are 
implemented in classes in the codebase, the first two in a class called \code{MCTSManager} 
and the latter in \code{OpponentBot}.  Additional classes are included for parsing the text 
input from the game engine and for storing the state of gameplay.  The input parser, basic 
outline of the bot code, and the classes for watching the bot state are all based on the 
starter code provided by the Warlight developers, referred to as StarterBot.  In an effort
to focus on the algorithms, we chose to use a tree structure library called \emph{tree.hh}\cite{treehh}.
As it was the primary objective for this work, we have included screenshots of our UCT
implementation in Appendix.  %%%%


\section{Experiments}\label{sec:experiments}
The simplest experiment for our bot was to submit it to the challenge server and see how it 
fared against other bots.  We have tested our bot against multiple other bots participating 
in the challenge, and we have won against several of them.  However, some of this success is 
due to the fact that many of the bots that have been uploaded are likely incomplete or are 
even just the unmodified StarterBot, and these results may be unreliable.

We are building an experiment in which we pit our MCTS/UCT bot against our greedy 
OpponentBot to determine how much better we can do with the intelligent bot than the greedy 
one.  However, we do not yet have results of this match.

Currently, the greedy agent performs well as a stand-alone agent (in test environment) 
independent of UCT; however it is currently still going through some integration issues with 
UCT. Originally developed to be run-time inexpensive, so that UCT implementation might make 
the most of pre-allocated runtime constraints imposed by game engine, the greedy algorithm 
was not designed with the design flexibility of modern object oriented language and was more 
tightly coupled by multiple container stacks and synchronizing event handler type routines. 
Thus complete integration of state classes employed by parent UCT algorithm is taking a 
little more time than the proposed time frame and remains a work in progress. However 
tangential the development of the DFS algorithm might have been within the scope of an UCT 
algorithm, the appeal of developing complementary planning algorithm as an academic exercise 
was simply too appealing to ignore.

\section{Conclusion}\label{sec:conclusion}
\todo{still needs to be filled in\ldots}

\bibliographystyle{plain}
\bibliography{references}

\newpage

\begin{appendices}
\section{Implementation Screenshots}
Below is a few screenshots of the UCT algorithm.


%
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.72\columnwidth]{uct_implementation}
  \caption{The UCT function definition that repeatedly calls the UCTHelper function as long as time permits.}
  \label{fig:uct_imp}
\end{figure}
%

%
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.72\columnwidth]{uct_helper_implementation}
  \caption{The UCT helper function that implementes the pseudo code shown in Figure \ref{fig:uct_alg}.}
  \label{fig:uct_imp}
\end{figure}
%



\end{appendices}



\end{document}
